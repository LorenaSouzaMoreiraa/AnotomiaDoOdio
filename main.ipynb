{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b074c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (3.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting pt-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n",
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
      "  Building wheel for liac-arff (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11768 sha256=2a8b548353c3e4987e01727376a81d6406aa33041f790d2d3ee15385e9016aee\n",
      "  Stored in directory: /home/loli/.cache/pip/wheels/a9/ac/cf/c2919807a5c623926d217c0a18eb5b457e5c19d242c3b5963a\n",
      "Successfully built liac-arff\n",
      "Installing collected packages: liac-arff\n",
      "Successfully installed liac-arff-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas networkx matplotlib spacy scipy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!pip install liac-arff\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382d5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd              # Para manipular os dados (CSVs, DataFrames)\n",
    "import re                        # Para usar expressões regulares na limpeza do texto (remover @)\n",
    "import networkx as nx            # Para criar e manipular os grafos\n",
    "import matplotlib.pyplot as plt  # Para visualizar os grafos\n",
    "import spacy                     # A biblioteca principal para processamento de linguagem em português\n",
    "from itertools import combinations\n",
    "import spacy\n",
    "import arff\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5de36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas essenciais e modelo 'pt_core_news_lg' do spaCy carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "# Isso substitui os downloads do NLTK\n",
    "try:\n",
    "    nlp = spacy.load('pt_core_news_lg')\n",
    "    print(\"Bibliotecas essenciais e modelo 'pt_core_news_lg' do spaCy carregados com sucesso!\")\n",
    "except OSError:\n",
    "    print(\"Modelo 'pt_core_news_sm' não encontrado. Por favor, execute a célula de instalação:\")\n",
    "    print(\"!python -m spacy download pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28824a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pré-visualização dos dados do .arff ---\n",
      "  @@class                                           document\n",
      "0     yes               Votaram no PEZAO Agora tomem no CZAO\n",
      "1      no  cuidado com a poupanca pessoal Lembram o que a...\n",
      "2      no  Sabe o que eu acho engracado os nossos governa...\n",
      "3      no              Podiam retirar dos lucros dos bancos \n",
      "4      no  CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...\n",
      "\n",
      "[SUCESSO] Arquivo './data/01-raw/offComBR/OffComBR3.arff' convertido para './data/01-raw/offComBR/offComBR.csv'\n"
     ]
    }
   ],
   "source": [
    "offComBR_01 = './data/01-raw/offComBR/OffComBR3.arff'\n",
    "# Carrega o arquivo .arff\n",
    "# Carrega o ARFF com liac-arff\n",
    "with open(offComBR_01, 'r', encoding='utf-8') as f:\n",
    "    arff_data = arff.load(f)\n",
    "\n",
    "# Converte para DataFrame\n",
    "df_arff = pd.DataFrame(arff_data['data'], columns=[attr[0] for attr in arff_data['attributes']])\n",
    "\n",
    "print(\"--- Pré-visualização dos dados do .arff ---\")\n",
    "print(df_arff.head())\n",
    "\n",
    "caminho_arff_saida = './data/01-raw/offComBR/offComBR.csv'\n",
    "# Salva o DataFrame como .csvcaminho_csv_saida_1\n",
    "df_arff.to_csv('./data/01-raw/offComBR/offComBR.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n[SUCESSO] Arquivo '{offComBR_01}' convertido para '{caminho_arff_saida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da93344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pré-visualização dos dados do .parquet ---\n",
      "                                 id  \\\n",
      "0  da19df36730945f08df3d09efa354876   \n",
      "1  80f1a8c981864887b13963fed1261acc   \n",
      "2  80eee9db811c4ea4b2ddb7863d12c5fe   \n",
      "3  2f67025f913e4a6292e3d000d9e2b5a8   \n",
      "4  e64148caa4474fc79298e01d0dda8f5e   \n",
      "\n",
      "                                                text is_offensive is_targeted  \\\n",
      "0  USER Adorei o comercial também Jesus. Só achei...          OFF         UNT   \n",
      "1  Cara isso foi muito babaca geral USER conhece ...          OFF         TIN   \n",
      "2                           Quem liga pra judeu kkkk          OFF         UNT   \n",
      "3  Se vc for porco, folgado e relaxado, você não ...          OFF         UNT   \n",
      "4    USER Toma no cu é vitamina como tu e tua prima.          OFF         TIN   \n",
      "\n",
      "  targeted_type                                        toxic_spans  health  \\\n",
      "0          None  [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...   False   \n",
      "1           GRP  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "2          None  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "3          None  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "4           GRP  [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...   False   \n",
      "\n",
      "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
      "0     False    True        False            False             False   \n",
      "1     False    True        False            False             False   \n",
      "2      True    True        False            False             False   \n",
      "3     False    True        False            False             False   \n",
      "4     False    True        False            False             False   \n",
      "\n",
      "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
      "0               True   False                  False   False       False  \n",
      "1              False   False                  False   False       False  \n",
      "2              False   False                  False   False        True  \n",
      "3              False   False                  False   False       False  \n",
      "4               True   False                  False   False       False  \n",
      "\n",
      "[SUCESSO] Arquivo './data/01-raw/olidBR/test-00000-of-00001-914dbee7561d2266.parquet' convertido para './data/01-raw/olidBR/olidBR.csv'\n"
     ]
    }
   ],
   "source": [
    "olidBR_01 = './data/01-raw/olidBR/test-00000-of-00001-914dbee7561d2266.parquet'\n",
    "df_parquet = pd.read_parquet(olidBR_01)\n",
    "print(\"--- Pré-visualização dos dados do .parquet ---\")\n",
    "print(df_parquet.head())\n",
    "\n",
    "caminho_parquet_saida = './data/01-raw/olidBR/olidBR.csv'\n",
    "# Salva o DataFrame como .csv\n",
    "df_parquet.to_csv(caminho_parquet_saida, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n[SUCESSO] Arquivo '{olidBR_01}' convertido para '{caminho_parquet_saida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28998fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total de instâncias por dataset ---\n",
      "\n",
      "hateBR (7000 instâncias):\n",
      "['id', 'comment', 'offensive_label', 'link_post', 'rationales_annotator1', 'rationales_annotator2']\n",
      "\n",
      "offComBR (1033 instâncias):\n",
      "['class', 'document']\n",
      "\n",
      "olidBR (1738 instâncias):\n",
      "['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia']\n",
      "\n",
      "fortuna (5670 instâncias):\n",
      "['text', 'hatespeech_comb', 'hatespeech_G1', 'annotator_G1', 'hatespeech_G2', 'annotator_G2', 'hatespeech_G3', 'annotator_G3']\n",
      "\n",
      "toLDBR (21000 instâncias):\n",
      "['text', 'homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia']\n",
      "\n",
      "tuPyE (8734 instâncias):\n",
      "['source', 'text', 'researcher', 'year', 'aggressive', 'hate']\n"
     ]
    }
   ],
   "source": [
    "datasets_01 = {\n",
    "    \"hateBR\": './data/01-raw/hateBR/HateBRXplain.csv',\n",
    "    \"offComBR\": './data/01-raw/offComBR/offComBR.csv',\n",
    "    \"olidBR\": './data/01-raw/olidBR/olidBR.csv',\n",
    "    \"fortuna\": './data/01-raw/fortuna/2019-05-28_portuguese_hate_speech_binary_classification.csv',\n",
    "    \"toLDBR\": './data/01-raw/toLDBR/ToLD-BR.csv',\n",
    "    \"tuPyE\": './data/01-raw/tuPyE/binary_test.csv'\n",
    "}\n",
    "\n",
    "# Lê e conta as instâncias\n",
    "print(\"--- Total de instâncias por dataset ---\")\n",
    "for nome, caminho in datasets_01.items():\n",
    "    try:\n",
    "        df = pd.read_csv(caminho)\n",
    "        print(f\"\\n{nome} ({df.shape[0]} instâncias):\")\n",
    "        print(list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"{nome}: Erro ao ler o arquivo - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hateBR: 3500 instâncias na offensive_label\n",
      "olidBR: 1484 instâncias na is_offensive\n",
      "fortuna: 1788 instâncias na hatespeech_comb\n",
      "toLDBR: 9255 instâncias na hate\n",
      "tuPyE: 1051 instâncias na hate\n"
     ]
    }
   ],
   "source": [
    "def cont_categorias(coluna_alvo, nome, value):\n",
    "    df = pd.read_csv(datasets_01[nome])\n",
    "    total = (df[coluna_alvo] == value).sum()\n",
    "    print(f\"{nome}: {total} instâncias na {coluna_alvo}\")\n",
    "\n",
    "cont_categorias('offensive_label','hateBR', 1)\n",
    "cont_categorias('is_offensive','olidBR', 'OFF')\n",
    "cont_categorias('hatespeech_comb','fortuna', 1)\n",
    "\n",
    "colunas_odio = ['homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia']\n",
    "df = pd.read_csv(datasets_01['toLDBR'])\n",
    "df['hate'] = df[colunas_odio].any(axis=1).astype(int)\n",
    "df.to_csv('./data/01-raw/toLDBR/ToLD-BR.csv', index=False, encoding='utf-8')\n",
    "cont_categorias('hate','toLDBR', 1)\n",
    "\n",
    "cont_categorias('hate','tuPyE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6d02d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_02 = {\n",
    "     \"hateBR\": {\n",
    "        'col_mensagem': 'comment',\n",
    "        'col_rotulo': 'offensive_label',\n",
    "        'saida': './data/02-cleaned/hateBR/HateBRXplain.csv'\n",
    "    },\n",
    "    \"offComBR\": {\n",
    "        'col_mensagem': 'document',             \n",
    "        'col_rotulo': 'class',\n",
    "        'saida': './data/02-cleaned/offComBR/offComBR.csv'\n",
    "    },\n",
    "    \"olidBR\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'is_offensive',\n",
    "        'saida': './data/02-cleaned/olidBR/olidBR.csv'\n",
    "    },\n",
    "    \"fortuna\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hatespeech_comb',\n",
    "        'saida': './data/02-cleaned/fortuna/fortuna.csv'\n",
    "    },\n",
    "    \"toLDBR\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hate',\n",
    "        'saida': './data/02-cleaned/toLDBR/ToLD-BR.csv'\n",
    "    },\n",
    "    \"tuPyE\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hate',\n",
    "        'saida': './data/02-cleaned/tuPyE/tuPyE.csv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de067805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] hateBR: 7000 instâncias salvas em './data/02-cleaned/hateBR/HateBRXplain.csv/hateBR.csv'\n",
      "[✓] offComBR: 1033 instâncias salvas em './data/02-cleaned/offComBR/offComBR.csv/offComBR.csv'\n",
      "[✓] olidBR: 1738 instâncias salvas em './data/02-cleaned/olidBR/olidBR.csv/olidBR.csv'\n",
      "[✓] fortuna: 5670 instâncias salvas em './data/02-cleaned/fortuna/fortuna.csv/fortuna.csv'\n",
      "[✓] toLDBR: 21000 instâncias salvas em './data/02-cleaned/toLDBR/ToLD-BR.csv/toLDBR.csv'\n",
      "[✓] tuPyE: 8734 instâncias salvas em './data/02-cleaned/tuPyE/tuPyE.csv/tuPyE.csv'\n"
     ]
    }
   ],
   "source": [
    "def uniformizar_csv(nome, caminho, col_mensagem, col_rotulo, saida):\n",
    "    try:\n",
    "        df = pd.read_csv(caminho)\n",
    "\n",
    "        # Verifica se as colunas estão no DataFrame\n",
    "        if col_mensagem in df.columns and col_rotulo in df.columns:\n",
    "            df_padronizado = df[[col_mensagem, col_rotulo]].rename(columns={\n",
    "                col_mensagem: 'mensagem',\n",
    "                col_rotulo: 'odio'\n",
    "            })\n",
    "\n",
    "            # Caminho de saída\n",
    "            os.makedirs(os.path.dirname(saida), exist_ok=True)\n",
    "            caminho_saida = os.path.join(saida, f\"{nome}.csv\")\n",
    "            df_padronizado.to_csv(saida, index=False, encoding='utf-8')\n",
    "\n",
    "            print(f\"[✓] {nome}: {df_padronizado.shape[0]} instâncias salvas em '{caminho_saida}'\")\n",
    "        else:\n",
    "            print(f\"[✗] {nome}: colunas '{col_mensagem}' e/ou '{col_rotulo}' não encontradas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] {nome}: {e}\")\n",
    "        \n",
    "for nome, config in datasets_02.items():\n",
    "    uniformizar_csv(\n",
    "        nome,\n",
    "        datasets_01[nome],\n",
    "        config['col_mensagem'],\n",
    "        config['col_rotulo'],\n",
    "        config['saida']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_e_lematizar(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpa o texto e aplica lematização com spaCy para português.\n",
    "    Remove menções, hashtags, emojis, stopwords e pontuações.\n",
    "    \"\"\"\n",
    "    # Remover menções (@usuario)\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Remover hashtags (#palavra) mas manter a palavra\n",
    "    texto = re.sub(r'#', '', texto)\n",
    "    # Remover emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Símbolos e pictogramas\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transporte e mapas\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Bandeiras\n",
    "        u\"\\U00002500-\\U00002BEF\"  # Caracteres diversos\n",
    "        u\"\\U00002700-\\U000027BF\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    texto = emoji_pattern.sub(r'', texto)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Lematizar, ignorando pontuação, stopwords e espaços\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67497bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processar_pt(caminho_csv: str, nome_coluna: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carrega um CSV, remove duplicatas e aplica limpeza e lematização otimizada\n",
    "    para o português em uma coluna de texto.\n",
    "    \"\"\"\n",
    "    # 1. Carregar o dataset\n",
    "    df = pd.read_csv(caminho_csv)\n",
    "    print(f\"Dataset original carregado com {len(df)} linhas.\")\n",
    "\n",
    "    # 2. Remover duplicatas\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Dataset após remoção de duplicatas possui {len(df)} linhas.\")\n",
    "\n",
    "    # 3. Limpeza e Lematização com spaCy\n",
    "    # Criar uma nova coluna para o texto processado\n",
    "    df['texto_processado'] = df[nome_coluna].apply(lambda texto: limpar_e_lematizar_spacy(texto))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f12129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função de montar o grafo permanece a mesma\n",
    "def montar_grafo_de_coocorrencia(series_texto: pd.Series) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Cria um grafo NetworkX a partir de uma série de textos processados.\n",
    "    Nós = palavras (lemas)\n",
    "    Arestas = coocorrência de palavras na mesma sentença (tweet/comentário)\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for texto in series_texto:\n",
    "        palavras = texto.split()\n",
    "        \n",
    "        # Adiciona nós (evita duplicatas automaticamente)\n",
    "        for palavra in palavras:\n",
    "            G.add_node(palavra)\n",
    "            \n",
    "        # Adiciona arestas baseadas nas combinações de palavras no mesmo texto\n",
    "        for w1, w2 in combinations(palavras, 2):\n",
    "            if G.has_edge(w1, w2):\n",
    "                if 'weight' in G[w1][w2]:\n",
    "                    G[w1][w2]['weight'] += 1\n",
    "                else:\n",
    "                     G[w1][w2]['weight'] = 2\n",
    "            else:\n",
    "                G.add_edge(w1, w2, weight=1)\n",
    "                \n",
    "    return G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
