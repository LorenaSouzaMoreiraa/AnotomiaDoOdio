{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b074c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (3.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting pt-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n",
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
      "  Building wheel for liac-arff (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11768 sha256=2a8b548353c3e4987e01727376a81d6406aa33041f790d2d3ee15385e9016aee\n",
      "  Stored in directory: /home/loli/.cache/pip/wheels/a9/ac/cf/c2919807a5c623926d217c0a18eb5b457e5c19d242c3b5963a\n",
      "Successfully built liac-arff\n",
      "Installing collected packages: liac-arff\n",
      "Successfully installed liac-arff-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas networkx matplotlib spacy scipy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!pip install liac-arff\n",
    "!pip install pyarrow\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382d5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd              # Para manipular os dados (CSVs, DataFrames)\n",
    "import re                        # Para usar expressões regulares na limpeza do texto (remover @)\n",
    "import networkx as nx            # Para criar e manipular os grafos\n",
    "import matplotlib.pyplot as plt  # Para visualizar os grafos\n",
    "import spacy                     # A biblioteca principal para processamento de linguagem em português\n",
    "from itertools import combinations\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "import arff\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5de36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas essenciais e modelo 'pt_core_news_lg' do spaCy carregados com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/loli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "# Isso substitui os downloads do NLTK\n",
    "try:\n",
    "    nlp = spacy.load('pt_core_news_lg')\n",
    "    print(\"Bibliotecas essenciais e modelo 'pt_core_news_lg' do spaCy carregados com sucesso!\")\n",
    "except OSError:\n",
    "    print(\"Modelo 'pt_core_news_sm' não encontrado. Por favor, execute a célula de instalação:\")\n",
    "    print(\"!python -m spacy download pt_core_news_lg\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28824a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pré-visualização dos dados do .arff ---\n",
      "  @@class                                           document\n",
      "0     yes               Votaram no PEZAO Agora tomem no CZAO\n",
      "1      no  cuidado com a poupanca pessoal Lembram o que a...\n",
      "2      no  Sabe o que eu acho engracado os nossos governa...\n",
      "3      no              Podiam retirar dos lucros dos bancos \n",
      "4      no  CADE O GALVAO PRA NARRAR AGORA   FALIIIIUUUUUU...\n",
      "\n",
      "[SUCESSO] Arquivo './data/01-raw/offComBR/OffComBR3.arff' convertido para './data/01-raw/offComBR/offComBR.csv'\n"
     ]
    }
   ],
   "source": [
    "offComBR_01 = './data/01-raw/offComBR/OffComBR3.arff'\n",
    "# Carrega o arquivo .arff\n",
    "# Carrega o ARFF com liac-arff\n",
    "with open(offComBR_01, 'r', encoding='utf-8') as f:\n",
    "    arff_data = arff.load(f)\n",
    "\n",
    "# Converte para DataFrame\n",
    "df_arff = pd.DataFrame(arff_data['data'], columns=[attr[0] for attr in arff_data['attributes']])\n",
    "\n",
    "print(\"--- Pré-visualização dos dados do .arff ---\")\n",
    "print(df_arff.head())\n",
    "\n",
    "caminho_arff_saida = './data/01-raw/offComBR/offComBR.csv'\n",
    "# Salva o DataFrame como .csvcaminho_csv_saida_1\n",
    "df_arff.to_csv('./data/01-raw/offComBR/offComBR.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n[SUCESSO] Arquivo '{offComBR_01}' convertido para '{caminho_arff_saida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da93344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pré-visualização dos dados do .parquet ---\n",
      "                                 id  \\\n",
      "0  da19df36730945f08df3d09efa354876   \n",
      "1  80f1a8c981864887b13963fed1261acc   \n",
      "2  80eee9db811c4ea4b2ddb7863d12c5fe   \n",
      "3  2f67025f913e4a6292e3d000d9e2b5a8   \n",
      "4  e64148caa4474fc79298e01d0dda8f5e   \n",
      "\n",
      "                                                text is_offensive is_targeted  \\\n",
      "0  USER Adorei o comercial também Jesus. Só achei...          OFF         UNT   \n",
      "1  Cara isso foi muito babaca geral USER conhece ...          OFF         TIN   \n",
      "2                           Quem liga pra judeu kkkk          OFF         UNT   \n",
      "3  Se vc for porco, folgado e relaxado, você não ...          OFF         UNT   \n",
      "4    USER Toma no cu é vitamina como tu e tua prima.          OFF         TIN   \n",
      "\n",
      "  targeted_type                                        toxic_spans  health  \\\n",
      "0          None  [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...   False   \n",
      "1           GRP  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "2          None  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "3          None  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
      "4           GRP  [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...   False   \n",
      "\n",
      "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
      "0     False    True        False            False             False   \n",
      "1     False    True        False            False             False   \n",
      "2      True    True        False            False             False   \n",
      "3     False    True        False            False             False   \n",
      "4     False    True        False            False             False   \n",
      "\n",
      "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
      "0               True   False                  False   False       False  \n",
      "1              False   False                  False   False       False  \n",
      "2              False   False                  False   False        True  \n",
      "3              False   False                  False   False       False  \n",
      "4               True   False                  False   False       False  \n",
      "\n",
      "[SUCESSO] Arquivo './data/01-raw/olidBR/test-00000-of-00001-914dbee7561d2266.parquet' convertido para './data/01-raw/olidBR/olidBR.csv'\n"
     ]
    }
   ],
   "source": [
    "olidBR_01 = './data/01-raw/olidBR/test-00000-of-00001-914dbee7561d2266.parquet'\n",
    "df_parquet = pd.read_parquet(olidBR_01)\n",
    "print(\"--- Pré-visualização dos dados do .parquet ---\")\n",
    "print(df_parquet.head())\n",
    "\n",
    "caminho_parquet_saida = './data/01-raw/olidBR/olidBR.csv'\n",
    "# Salva o DataFrame como .csv\n",
    "df_parquet.to_csv(caminho_parquet_saida, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n[SUCESSO] Arquivo '{olidBR_01}' convertido para '{caminho_parquet_saida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28998fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Total de instâncias por dataset ---\n",
      "\n",
      "hateBR (7000 instâncias):\n",
      "['id', 'comment', 'offensive_label', 'link_post', 'rationales_annotator1', 'rationales_annotator2']\n",
      "\n",
      "offComBR (1033 instâncias):\n",
      "['class', 'document']\n",
      "\n",
      "olidBR (1738 instâncias):\n",
      "['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia']\n",
      "\n",
      "fortuna (5670 instâncias):\n",
      "['text', 'hatespeech_comb', 'hatespeech_G1', 'annotator_G1', 'hatespeech_G2', 'annotator_G2', 'hatespeech_G3', 'annotator_G3']\n",
      "\n",
      "toLDBR (21000 instâncias):\n",
      "['text', 'homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia', 'hate']\n",
      "\n",
      "tuPyE (8734 instâncias):\n",
      "['source', 'text', 'researcher', 'year', 'aggressive', 'hate']\n"
     ]
    }
   ],
   "source": [
    "datasets_01 = {\n",
    "    \"hateBR\": './data/01-raw/hateBR/HateBRXplain.csv',\n",
    "    \"offComBR\": './data/01-raw/offComBR/offComBR.csv',\n",
    "    \"olidBR\": './data/01-raw/olidBR/olidBR.csv',\n",
    "    \"fortuna\": './data/01-raw/fortuna/2019-05-28_portuguese_hate_speech_binary_classification.csv',\n",
    "    \"toLDBR\": './data/01-raw/toLDBR/ToLD-BR.csv',\n",
    "    \"tuPyE\": './data/01-raw/tuPyE/binary_test.csv'\n",
    "}\n",
    "\n",
    "# Lê e conta as instâncias\n",
    "print(\"--- Total de instâncias por dataset ---\")\n",
    "for nome, caminho in datasets_01.items():\n",
    "    try:\n",
    "        df = pd.read_csv(caminho)\n",
    "        print(f\"\\n{nome} ({df.shape[0]} instâncias):\")\n",
    "        print(list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"{nome}: Erro ao ler o arquivo - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hateBR: 3500 instâncias na offensive_label\n",
      "olidBR: 1484 instâncias na is_offensive\n",
      "fortuna: 1788 instâncias na hatespeech_comb\n",
      "toLDBR: 9255 instâncias na hate\n",
      "tuPyE: 1051 instâncias na hate\n"
     ]
    }
   ],
   "source": [
    "def cont_categorias(coluna_alvo, nome, value):\n",
    "    df = pd.read_csv(datasets_01[nome])\n",
    "    total = (df[coluna_alvo] == value).sum()\n",
    "    print(f\"{nome}: {total} instâncias na {coluna_alvo}\")\n",
    "\n",
    "cont_categorias('offensive_label','hateBR', 1)\n",
    "cont_categorias('is_offensive','olidBR', 'OFF')\n",
    "cont_categorias('hatespeech_comb','fortuna', 1)\n",
    "\n",
    "colunas_odio = ['homophobia', 'obscene', 'insult', 'racism', 'misogyny', 'xenophobia']\n",
    "df = pd.read_csv(datasets_01['toLDBR'])\n",
    "df['hate'] = df[colunas_odio].any(axis=1).astype(int)\n",
    "df.to_csv('./data/01-raw/toLDBR/ToLD-BR.csv', index=False, encoding='utf-8')\n",
    "cont_categorias('hate','toLDBR', 1)\n",
    "\n",
    "cont_categorias('hate','tuPyE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d02d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_02 = {\n",
    "     \"hateBR\": {\n",
    "        'col_mensagem': 'comment',\n",
    "        'col_rotulo': 'offensive_label',\n",
    "        'saida': './data/02-cleaned/hateBR/HateBRXplain.csv'\n",
    "    },\n",
    "    \"offComBR\": {\n",
    "        'col_mensagem': 'document',             \n",
    "        'col_rotulo': 'class',\n",
    "        'saida': './data/02-cleaned/offComBR/offComBR.csv'\n",
    "    },\n",
    "    \"olidBR\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'is_offensive',\n",
    "        'saida': './data/02-cleaned/olidBR/olidBR.csv'\n",
    "    },\n",
    "    \"fortuna\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hatespeech_comb',\n",
    "        'saida': './data/02-cleaned/fortuna/fortuna.csv'\n",
    "    },\n",
    "    \"toLDBR\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hate',\n",
    "        'saida': './data/02-cleaned/toLDBR/ToLD-BR.csv'\n",
    "    },\n",
    "    \"tuPyE\": {\n",
    "        'col_mensagem': 'text',\n",
    "        'col_rotulo': 'hate',\n",
    "        'saida': './data/02-cleaned/tuPyE/tuPyE.csv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de067805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] hateBR: 7000 instâncias salvas em './data/02-cleaned/hateBR/HateBRXplain.csv/hateBR.csv'\n",
      "[✓] offComBR: 1033 instâncias salvas em './data/02-cleaned/offComBR/offComBR.csv/offComBR.csv'\n",
      "[✓] olidBR: 1738 instâncias salvas em './data/02-cleaned/olidBR/olidBR.csv/olidBR.csv'\n",
      "[✓] fortuna: 5670 instâncias salvas em './data/02-cleaned/fortuna/fortuna.csv/fortuna.csv'\n",
      "[✓] toLDBR: 21000 instâncias salvas em './data/02-cleaned/toLDBR/ToLD-BR.csv/toLDBR.csv'\n",
      "[✓] tuPyE: 8734 instâncias salvas em './data/02-cleaned/tuPyE/tuPyE.csv/tuPyE.csv'\n"
     ]
    }
   ],
   "source": [
    "def uniformizar_csv(nome, caminho, col_mensagem, col_rotulo, saida):\n",
    "    try:\n",
    "        df = pd.read_csv(caminho)\n",
    "\n",
    "        # Verifica se as colunas estão no DataFrame\n",
    "        if col_mensagem in df.columns and col_rotulo in df.columns:\n",
    "            df_padronizado = df[[col_mensagem, col_rotulo]].rename(columns={\n",
    "                col_mensagem: 'mensagem',\n",
    "                col_rotulo: 'odio'\n",
    "            })\n",
    "\n",
    "            # Caminho de saída\n",
    "            os.makedirs(os.path.dirname(saida), exist_ok=True)\n",
    "            caminho_saida = os.path.join(saida, f\"{nome}.csv\")\n",
    "            df_padronizado.to_csv(saida, index=False, encoding='utf-8')\n",
    "\n",
    "            print(f\"[✓] {nome}: {df_padronizado.shape[0]} instâncias salvas em '{caminho_saida}'\")\n",
    "        else:\n",
    "            print(f\"[✗] {nome}: colunas '{col_mensagem}' e/ou '{col_rotulo}' não encontradas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] {nome}: {e}\")\n",
    "        \n",
    "for nome, config in datasets_02.items():\n",
    "    uniformizar_csv(\n",
    "        nome,\n",
    "        datasets_01[nome],\n",
    "        config['col_mensagem'],\n",
    "        config['col_rotulo'],\n",
    "        config['saida']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67497bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_03 = {\n",
    "    \"hateBR\": './data/03-processed/hateBR/HateBRXplain.csv',\n",
    "    \"offComBR\": './data/03-processed/offComBR/offComBR.csv',\n",
    "    \"olidBR\": './data/03-processed/olidBR/olidBR.csv',\n",
    "    \"fortuna\": './data/03-processed/fortuna/fortuna.csv',\n",
    "    \"toLDBR\": './data/03-processed/toLDBR/ToLD-BR.csv',\n",
    "    \"tuPyE\": './data/03-processed/tuPyE/tuPyE.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d2a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_e_lematizar(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpa o texto e aplica lematização com spaCy para português.\n",
    "    Remove menções, hashtags, emojis, stopwords e pontuações.\n",
    "    \"\"\"\n",
    "  # Remove URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+', '', texto)\n",
    "    # Remove menções (@usuario)\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Remove hashtags (#assunto)\n",
    "    texto = re.sub(r'#\\w+', '', texto)\n",
    "    # Remove a marcação de \"RT\" (Retweet)\n",
    "    texto = re.sub(r'RT', '', texto)\n",
    "    # Remove caracteres de nova linha (\\n) e os substitui por espaço\n",
    "    texto = re.sub(r'\\n', ' ', texto)\n",
    "    # Remove caracteres especiais, pontuações e números, deixando apenas letras e espaços\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "    # Remove espaços em branco extras (deixa apenas um espaço entre as palavras)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Símbolos e pictogramas\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transporte e mapas\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Bandeiras\n",
    "        u\"\\U00002500-\\U00002BEF\"  # Caracteres diversos\n",
    "        u\"\\U00002700-\\U000027BF\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    texto = emoji_pattern.sub(r'', texto)\n",
    "\n",
    "    # --- Remoção de Stop Words ---\n",
    "    # 1. Converte o texto para minúsculas e divide em palavras (tokens)\n",
    "    palavras = texto.lower().split()\n",
    "\n",
    "    # Processa o texto com o modelo spaCy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # 2. Remove as stop words e lematização\n",
    "    lemmas = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "    ]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[→] Processando 'hateBR' (7000 linhas)...\n",
      "[✓] 'hateBR' salvo em ./data/03-processed/hateBR/HateBRXplain.csv com coluna 'texto_processado'.\n",
      "\n",
      "[→] Processando 'offComBR' (1033 linhas)...\n",
      "[✓] 'offComBR' salvo em ./data/03-processed/offComBR/offComBR.csv com coluna 'texto_processado'.\n",
      "\n",
      "[→] Processando 'olidBR' (1738 linhas)...\n",
      "[✓] 'olidBR' salvo em ./data/03-processed/olidBR/olidBR.csv com coluna 'texto_processado'.\n",
      "\n",
      "[→] Processando 'fortuna' (5670 linhas)...\n",
      "[✓] 'fortuna' salvo em ./data/03-processed/fortuna/fortuna.csv com coluna 'texto_processado'.\n",
      "\n",
      "[→] Processando 'toLDBR' (21000 linhas)...\n",
      "[✓] 'toLDBR' salvo em ./data/03-processed/toLDBR/ToLD-BR.csv com coluna 'texto_processado'.\n",
      "\n",
      "[→] Processando 'tuPyE' (8734 linhas)...\n",
      "[✓] 'tuPyE' salvo em ./data/03-processed/tuPyE/tuPyE.csv com coluna 'texto_processado'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nome in datasets_03:\n",
    "    caminho_entrada = datasets_02[nome]['saida']\n",
    "    caminho_saida = datasets_03[nome]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(caminho_entrada)\n",
    "\n",
    "        if 'mensagem' not in df.columns:\n",
    "            print(f\"[⚠️] {nome}: coluna 'mensagem' não encontrada. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[→] Processando '{nome}' ({len(df)} linhas)...\")\n",
    "\n",
    "        df['mensagem'] = df['mensagem'].astype(str).apply(limpar_e_lematizar)\n",
    "\n",
    "        diretorio = os.path.dirname(caminho_saida)\n",
    "        if diretorio:\n",
    "            os.makedirs(diretorio, exist_ok=True)\n",
    "\n",
    "        df.to_csv(caminho_saida, index=False, encoding='utf-8')\n",
    "        print(f\"[✓] '{nome}' salvo em {caminho_saida} .\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] ao processar '{nome}': {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38bdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatebase: 3901 instâncias\n"
     ]
    }
   ],
   "source": [
    "hurtlex_01 = pd.read_csv('./data/01-raw/hurtlex/hurtlex_PT.tsv', sep='\\t')\n",
    "caminho_saida = './data/02-cleaned/hurtlex/hurtlex.csv'\n",
    "diretorio = os.path.dirname(caminho_saida)\n",
    "if diretorio:\n",
    "    os.makedirs(diretorio, exist_ok=True)\n",
    "hurtlex_01.to_csv(caminho_saida, index=False)\n",
    "print(f\"hatebase: {hurtlex_01['lemma'].count()} instâncias\")\n",
    "\n",
    "caminho_saida = './data/02-cleaned/hurtlex/hurtlex.csv'\n",
    "diretorio = os.path.dirname(caminho_saida)\n",
    "if diretorio:\n",
    "    os.makedirs(diretorio, exist_ok=True)\n",
    "hurtlex_01[hurtlex_01['level'] == 'conservative'].to_csv(caminho_saida, index=False)\n",
    "\n",
    "caminho_saida = './data/03-processed/hurtlex/hurtlex.csv'\n",
    "hurtlex_02 = pd.read_csv('./data/02-cleaned/hurtlex/hurtlex.csv')\n",
    "lemmas_processados = []\n",
    "for doc in nlp.pipe(hurtlex_02['lemma'].astype(str), batch_size=50):\n",
    "    palavras = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "    ]\n",
    "    lemmas_processados.append(\" \".join(palavras))\n",
    "hurtlex_02['lemma'] = lemmas_processados\n",
    "hurtlex_02.to_csv(caminho_saida, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47937425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatebase: 680 instâncias duplicadas\n",
      "hatebase: 892 instâncias após processamento\n"
     ]
    }
   ],
   "source": [
    "hurtlex_03 = pd.read_csv(caminho_saida)\n",
    "print(f\"hatebase: {hurtlex_03['lemma'].duplicated().sum()} instâncias duplicadas\")\n",
    "hatebase = hurtlex_03.groupby('lemma').agg('max').to_dict(orient='index')\n",
    "print(f\"hatebase: {len(hatebase)} instâncias após processamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f12129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função de montar o grafo permanece a mesma\n",
    "def grafo_de_coocorrencia(series_texto: pd.Series, hatebase: dict) -> nx.Graph:\n",
    "    categorias = ['ps', 'rci', 'pa', 'ddf', 'ddp', 'dmc', 'is', 'or', 'an', 'asm', 'asf', 'pr', 'om', 'qas', 'cds', 're', 'svp']\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for texto in series_texto:\n",
    "        palavras = texto.split()\n",
    "        \n",
    "        for palavra in palavras:\n",
    "            if palavra not in G:\n",
    "                if palavra in hatebase:\n",
    "                    attrs = {'odio': 1}\n",
    "                    for cat in categorias:\n",
    "                        attrs[cat] = int(hatebase[palavra].get(cat, 0))\n",
    "                else:\n",
    "                    attrs = {'odio': 0}\n",
    "                    for cat in categorias:\n",
    "                        attrs[cat] = 0\n",
    "                G.add_node(palavra, **attrs)\n",
    "        \n",
    "        for i in range(len(palavras) - 1):\n",
    "            w1, w2 = palavras[i], palavras[i+1]\n",
    "            if G.has_edge(w1, w2):\n",
    "                G[w1][w2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(w1, w2, weight=1)\n",
    "                \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ee8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_grafo(grafo: nx.Graph, nome_dataset: str, pasta_saida: str = './data/04-graph', formato: str = 'gexf'):\n",
    "    \"\"\"\n",
    "    Salva o grafo com nome baseado no dataset fornecido.\n",
    "    \n",
    "    Parâmetros:\n",
    "        grafo (nx.Graph): O grafo a ser salvo\n",
    "        nome_dataset (str): Nome base do dataset (ex: 'hurtlex', 'twitter_clean', etc.)\n",
    "        pasta_saida (str): Pasta onde o grafo será salvo\n",
    "        formato (str): Formato do arquivo ('graphml', 'gexf', 'pkl')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Garante que a pasta existe\n",
    "    os.makedirs(pasta_saida, exist_ok=True)\n",
    "\n",
    "    # Define nome do arquivo final\n",
    "    caminho_saida = os.path.join(pasta_saida, f'{nome_dataset}.{formato}')\n",
    "\n",
    "    # Converte atributos para tipos simples (por segurança)\n",
    "    for _, data in grafo.nodes(data=True):\n",
    "        for key in data:\n",
    "            if isinstance(data[key], (bool, float)):\n",
    "                data[key] = int(data[key])\n",
    "\n",
    "    # Salva no formato desejado\n",
    "    if formato == 'graphml':\n",
    "        nx.write_graphml(grafo, caminho_saida)\n",
    "    elif formato == 'gexf':\n",
    "        nx.write_gexf(grafo, caminho_saida)\n",
    "    elif formato == 'pkl':\n",
    "        import pickle\n",
    "        with open(caminho_saida, 'wb') as f:\n",
    "            pickle.dump(grafo, f)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato '{formato}' não suportado.\")\n",
    "\n",
    "    print(f\"Grafo salvo com sucesso em: {caminho_saida}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc197ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dataset: hateBR\n",
      "Grafo salvo com sucesso em: ./data/04-graph/hateBR.gexf\n",
      "Processando dataset: offComBR\n",
      "Grafo salvo com sucesso em: ./data/04-graph/offComBR.gexf\n",
      "Processando dataset: olidBR\n",
      "Grafo salvo com sucesso em: ./data/04-graph/olidBR.gexf\n",
      "Processando dataset: fortuna\n",
      "Grafo salvo com sucesso em: ./data/04-graph/fortuna.gexf\n",
      "Processando dataset: toLDBR\n",
      "Grafo salvo com sucesso em: ./data/04-graph/toLDBR.gexf\n",
      "Processando dataset: tuPyE\n",
      "Grafo salvo com sucesso em: ./data/04-graph/tuPyE.gexf\n"
     ]
    }
   ],
   "source": [
    "for nome, caminho in datasets_03.items():\n",
    "    print(f\"Processando dataset: {nome}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(caminho)\n",
    "\n",
    "        if 'mensagem' not in df.columns:\n",
    "            print(f\"⚠️  Coluna 'mensagem' não encontrada em: {nome}. Pulando...\")\n",
    "            continue\n",
    "        \n",
    "        # REVISÃO: Adicionar .dropna() para evitar erros se houver mensagens vazias (NaN).\n",
    "        mensagens_validas = df.dropna(subset=['mensagem'])['mensagem']\n",
    "\n",
    "        if mensagens_validas.empty:\n",
    "            print(f\"⚠️  Nenhuma mensagem válida encontrada em: {nome}. Pulando...\")\n",
    "            continue\n",
    "        \n",
    "        # Cria o grafo com a nova função\n",
    "        G = grafo_de_coocorrencia(mensagens_validas, hatebase)\n",
    "\n",
    "        # Salva o grafo\n",
    "        salvar_grafo(G, nome, formato='gexf')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ X ] Arquivo não encontrado para o dataset '{nome}' em: {caminho}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ X ] Ocorreu um erro inesperado ao processar '{nome}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_grafo(caminho_arquivo,usar_atributo='grau',multiplicador_no=20, multiplicador_aresta=1):\n",
    "    # Carregar o grafo\n",
    "    G = nx.read_gexf(caminho_arquivo)\n",
    "\n",
    "    if usar_atributo == 'grau':\n",
    "        node_sizes = [G.degree[n] * multiplicador_no for n in G.nodes]\n",
    "    else:\n",
    "        node_sizes = [G.nodes[n].get(usar_atributo, 0) * multiplicador_no for n in G.nodes]\n",
    "\n",
    "    # Largura das arestas proporcional ao peso (weight)\n",
    "    edge_widths = [G.edges[e].get('weight', 1) * multiplicador_aresta for e in G.edges]\n",
    "\n",
    "    # Layout do grafo\n",
    "    pos = nx.spring_layout(G, k=0.35, iterations=50)\n",
    "\n",
    "    # Desenhar o grafo\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue')\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', alpha=0.6)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "    plt.title(f\"Grafo de Coocorrência de Palavras ({usar_atributo})\", size=15)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Número de nós: {G.number_of_nodes()}\")\n",
    "    print(f\"Número de arestas: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6988a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Atributo 'count' ausente em alguns nós.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvisualizar_grafo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data/04-graph/fortuna.gexf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mvisualizar_grafo\u001b[39m\u001b[34m(caminho_arquivo)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Verificar se os atributos esperados estão presentes\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m G.nodes[n] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m G.nodes):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAtributo \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ausente em alguns nós.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m G.edges[e] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m G.edges):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAtributo \u001b[39m\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m\u001b[33m ausente em algumas arestas.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Atributo 'count' ausente em alguns nós."
     ]
    }
   ],
   "source": [
    "visualizar_grafo('./data/04-graph/fortuna.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
